{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xvAHGqRFioNS"
   },
   "source": [
    "# Deep Learning reproducability project\n",
    "\n",
    "## Deep learning with multimodal representation for pancancer prognosis prediction\n",
    "\n",
    "## Authors: Anika Cheerla, Olivier Gevaert\n",
    "\n",
    "## Authors of the reproduction: Luke Prananta, Joris Feijen, Favian Stelmach, Zsombor Csuvar (Group 37)\n",
    "\n",
    "### Short description of the article \n",
    "The goal of the article was to create a deep learning algorithm that is to give accurate predictions of the future course of patients with cancer. It would be able to do this by using large amounts of multimodal data which previously was not possible to be done by physicians.  Out of all different data types (clinical, genomic profiling, histoloy slide images and radiographic images) this article took into consideration the gene expression data, miRNA data, clinical data and the whole slide images.\n",
    "In preceding articles it was shown that these data types individually can be used to predict the prognosis with high accuracy. Using a combination of said features was mode possible by the creation of databases like \"The Cancer Genome Atlas (TCGA)\" which was the main source of information for this article.\n",
    "Using said information the authors of the article created an unsupervised deep learning model that aggregates the above described data into a single feature vector and makes an accurate prognosis (overall C-index of 0.78).\n",
    "\n",
    "### Composition of the original article\n",
    "\n",
    "The first problem that the article had to solve was how to use the data that is both heterogen and high dimensional in nature. The architecture has to be able to  Additionally the model also had to be able to cope with missing data as the information available per patient and per cancer type varied greatly.\n",
    "\n",
    "To solve these problems the author created four different models one for each of the different data types. These four deep neural network models would each output a feature vector that would later be combined together using similarity loss to a single feature vector. To predict the survival data the concordance score was maximized using the Cox loss function. \n",
    "\n",
    "### Description of the model architectures\n",
    "\n",
    "As mentioned earlier the deep learning model of the article consists of four different models that are then combined together using similarity loss and then the prognosis prediction is done using the Cox loss function.\n",
    "The four model architectures are based on CNN-s. For the clinical data fully connected data layers were used with sigmoid activation. The models for the gene expression and the miRNA maintained the basic structure of the clinical data model but a highway gate and dropout was added. \n",
    "Due to the size of the whole slide images (WSI) stochastic sampling was used to decrease their size, random patches of the image was chosen than it was fed to a SqueezeNet model. Similarly to the other models the output of this was a feature vector consisting of 512 elements.\n",
    " \n",
    "### Reproduction of the article\n",
    "\n",
    "The reproduction of the article presented several issues and difficulties. Although the authors provided the code for the experiments and data collection we were unable to use said code without modification or rather complete rewriting. Furthermore the code was lacking comments which made its understanding and reuse difficult.\n",
    "\n",
    "#### Steps of reproduction\n",
    "\n",
    "The reproduction consisted of the following steps:\n",
    " 1. Download the necessary clinical data (all four types)\n",
    " 2. Replication of the individual models for evaluating the gene expression datam miRNA data, clinical data and whole slide images\n",
    " 3. Reproduction of the results of the paper\n",
    "\n",
    "#### Download the necessary clinical data\n",
    "\n",
    "Out of the different types of medical data the handling of the whole slide images was the most difficult. This was due to their size. A single slide can be as big as several gigabytes and the GDC database [GDC database](https://portal.gdc.cancer.gov/repository?filters=%7B%22op%22%3A%22and%22%2C%22content%22%3A%5B%7B%22op%22%3A%22in%22%2C%22content%22%3A%7B%22field%22%3A%22files.data_type%22%2C%22value%22%3A%5B%22Slide%20Image%22%5D%7D%7D%5D%7D) currently holds more than 30 000 whole slide image files which is 16.98 TB in total. This amount of data was impossible download or store using our personal computers. Therefore based on the guidance from out supervisor (Soufiane Mouragui) we only used the slides where the primary site of the cancer was the skin. This meant that only 950 files (707 GB) of data needed to be downloaded. To this goal the original code needed to be modified (the code can be found in slide_download_convert.py). For this to correctly run a new manifest file is needed that only includes the skin cancer files.\n",
    "The other three data types could be downloaded using the instructions \n",
    "\n",
    "### Graph reproduction \n",
    "\n",
    "As written in the description of the assignment the figure 1 also had to be reproduce. Figure 1 represenets the Kaplan-Meier survival curves for all cancer sites. This shows that based on different cancer sites we can expect different survival rates. In the reproduction of said graph the difficulty was that by downloading the clinical data from the GDC database we do not get the primary site of the cancer, but the specific type of cancer. Also in this case it the data was incomplete in some cases and no indication was given of how missing data was handled. \n",
    "The resulting figure can be obtained by running the fig_1_reproduce.py\n",
    "Some differences can be seen on the reproduced figure and the original in the article. The are multiple possible explanations for this. The article was written in the end of 2018 (based on the Github commit) but it is unknown that when was the data downloaded from the GDC database, and because that is continously updated it is possible that at the moment we are working with patient records that were not present when the article was written. Another possible explanation might be the handling of the missing information. In the reproduction when information was missing then the record was disregarded but in the original work it also could have been handled differently, eg.: substituting it with an average/ maximum or minimum values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "yvMLHcoCioNX",
    "outputId": "bfbdbc19-761e-4732-fe65-b720324df146"
   },
   "outputs": [],
   "source": [
    "%matplotlib inline  \n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using Device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "H_EN8MXpUwew"
   },
   "outputs": [],
   "source": [
    "layer_size = 512\n",
    "np_type = 'float32'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "f23Y5He_ioNd"
   },
   "source": [
    "# Highway Network\n",
    "\n",
    "For the implementation of the highway network it was necessary to read the reference literature. The referenced articled detailed the structure of the highway network and using that it was possible to construct it. Furthermore it was also indicated the value of the bias term.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "86NqqltDioNd"
   },
   "outputs": [],
   "source": [
    "# Implemented as p\n",
    "class Highway(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Highway, self).__init__()\n",
    "        \n",
    "        # Transformation gate exactly as defined by the paper\n",
    "        self.transform_gate = nn.Linear(layer_size, layer_size)\n",
    "        # Authors of the paper recommend filling the bias term with negative values\n",
    "        self.transform_gate.bias.data.fill_(-2)\n",
    "        \n",
    "        # The authors say that:\n",
    "        # \"H is usually an affine transform followed by a non-linear activation function, but in general it may take other forms\"\n",
    "        # https://arxiv.org/pdf/1505.00387.pdf\n",
    "        self.affine = nn.Linear(layer_size, layer_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Sigmoid as recommended by authors\n",
    "        T = torch.sigmoid(self.transform_gate(x))\n",
    "        \n",
    "        # Any non-linear activation can be used here\n",
    "        H = F.relu(self.affine(x))\n",
    "        \n",
    "        return H * T + x * (1 - T)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "h3xgbG-WioNg"
   },
   "source": [
    "# Gene Expression Sub-Network\n",
    "\n",
    "When downloading the gene expression data from the GDC database using the provided code we faced difficulties the article stated that it had it had 10198 cases meanwhile in our case we only managed to download 5898 cases. The same issue was present with the miRNA data where instead of 10125 cases we just got 5149.\n",
    "Besides only accessing part of the data described in the article and further research on the highway networks the implementation of the gene expression and the miRNA sub-network was straightforward."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "uG5IItAqioNg"
   },
   "outputs": [],
   "source": [
    "class GenExp(nn.Module):\n",
    "    def __init__(self, n, dropout):\n",
    "        super(GenExp, self).__init__()\n",
    "        cycle_layers = []\n",
    "        for i in range(n):\n",
    "            l = 60483 if i == 0 else layer_size\n",
    "            cycle_layers.append(nn.Linear(l, layer_size))\n",
    "            cycle_layers.append(Highway())\n",
    "        self.cycle_layers = nn.Sequential(*cycle_layers)\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.cycle_layers(x)\n",
    "        x = self.dropout(x)\n",
    "        x = torch.sigmoid(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "HTd4l21PioNj"
   },
   "source": [
    "# miRNA Sub-Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "38jH6pGQioNk"
   },
   "outputs": [],
   "source": [
    "class miRNA(nn.Module):\n",
    "    def __init__(self, n, dropout):\n",
    "        super(miRNA, self).__init__()\n",
    "        cycle_layers = []\n",
    "        for i in range(n):\n",
    "            l = 1881 if i == 0 else layer_size\n",
    "            cycle_layers.append(nn.Linear(l, layer_size))\n",
    "            cycle_layers.append(Highway())\n",
    "        self.cycle_layers = nn.Sequential(*cycle_layers)\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.cycle_layers(x)\n",
    "        x = self.dropout(x)\n",
    "        x = torch.sigmoid(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "FVDYhEpzioNn"
   },
   "source": [
    "# Clinical Data Sub-Network\n",
    "\n",
    "Clinical data sub-network was the simplest one out of the four network its structure was well represented in figure 2 of the article."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "QUJZ1XZAioNn"
   },
   "outputs": [],
   "source": [
    "class Clinical(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Clinical, self).__init__()\n",
    "        self.fc = nn.Linear(4, layer_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc(x)\n",
    "        x = torch.sigmoid(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "n4Mm14wPioNq"
   },
   "source": [
    "# Whole Slide Images Sub-Network\n",
    "The reproduction of the algorithm and all the steps necessary to implement this model was successful however due to the memory requirements posed by the fire modules used here our resources (desktop computer with 32GB of ram) were not sufficient. Therefore this module was not used in the analysis. \n",
    "Furthermore there were several issues when designing the network. While reproducing the network to encode the WSI into feature vectors, it was necessary to look into the paper used to make the SqueezeNet. Furthermore, a look into the code in the Git repository was necessary to find the exact parameters. Also there is an unmentioned difference between the original SqueezeNet and the used SqueezeNet: the last pooling layer in the original paper uses average pooling, while in the paper in question they use may pooling. Furthermore, because the network processes a batch of 40 samples for each image, it outputs a feature vector for each image. This should be one feature vector since it originates from one image, but the paper does not mention how to achieve this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "GhpARDjAioNr"
   },
   "outputs": [],
   "source": [
    "class fire(nn.Module):\n",
    "    def __init__(self, n_channels, s1x1, e1x1, e3x3):\n",
    "        super(fire,self).__init__()\n",
    "        self.squeeze = nn.Conv2d(n_channels,s1x1,1)\n",
    "        self.expand1 = nn.Conv2d(s1x1,e1x1,1)\n",
    "        self.expand3 = nn.Conv2d(s1x1,e3x3,3,padding=1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        out1 = F.relu(self.squeeze(x))\n",
    "        out2 = F.relu(self.expand1(out1))\n",
    "        out3 = F.relu(self.expand3(out1))\n",
    "        return torch.cat([out2,out3],1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "isnLyIghioNu"
   },
   "outputs": [],
   "source": [
    "class WholeSlide(nn.Module):\n",
    "    def __init__(self,n_channels):\n",
    "        super(WholeSlide,self).__init__()\n",
    "        self.conv1 = nn.Conv2d(n_channels,96,3,stride=2)\n",
    "        self.pool = nn.MaxPool2d(3,stride=2)\n",
    "        self.fire2 = fire(96,16,64,64)\n",
    "        self.fire3 = fire(128,16,64,64)\n",
    "        self.fire4 = fire(128,32,128,128)\n",
    "        self.fire5 = fire(256,32,128,128)\n",
    "        self.fire6 = fire(256,48,192,192)\n",
    "        self.fire7 = fire(384,48,192,192)\n",
    "        self.fire8 = fire(384,64,256,256)\n",
    "        self.fire9 = fire(512,64,256,256)\n",
    "        self.dropout = nn.Dropout2d(0.5)\n",
    "        self.conv10 = nn.Conv2d(512,1000,1)\n",
    "        self.fc = nn.Linear(1000*6*6*40,layer_size)\n",
    "    \n",
    "    def forward(self,x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.fire2(x)\n",
    "        x = self.fire3(x)\n",
    "        x = self.fire4(x)\n",
    "        x = self.pool(x)\n",
    "        x = self.fire5(x)\n",
    "        x = self.fire6(x)\n",
    "        x = self.fire7(x)\n",
    "        x = self.fire8(x)\n",
    "        x = self.pool(x)\n",
    "        x = self.fire9(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.pool(F.relu(self.conv10(x)))\n",
    "        x = x.view(1000*6*6*40)\n",
    "        x = self.fc(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dKKia_E7ioNw"
   },
   "source": [
    "# Main Network\n",
    "\n",
    "After excluding the WSI network from the reproduction the model only included three elements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Om30WKEJioNx"
   },
   "outputs": [],
   "source": [
    "from torch.distributions.bernoulli import Bernoulli\n",
    "\n",
    "class Network(nn.Module):\n",
    "    def __init__(self, p=0.25):\n",
    "        super(Network, self).__init__()\n",
    "        self.gen_exp = GenExp(10, 0.3)\n",
    "        self.mirna = miRNA(10, 0.3)\n",
    "        self.clinical = Clinical()\n",
    "        self.fc = nn.Linear(layer_size * 3, 1)\n",
    "        self.d = nn.Dropout2d(p=p)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x_gen_exp = self.gen_exp(x['gen_exp'])\n",
    "        x_mirna = self.mirna(x['mirna'])\n",
    "        x_clinical = self.clinical(x['clinical'])\n",
    "\n",
    "        # Perform multimodal dropout if p is not 0\n",
    "        x_gen_exp = self.d(x_gen_exp.unsqueeze(0)).squeeze()\n",
    "        x_mirna = self.d(x_mirna.unsqueeze(0)).squeeze()\n",
    "        x_clinical = self.d(x_clinical.unsqueeze(0)).squeeze()\n",
    "\n",
    "        # Here, the modalities are merged into a single tensor \n",
    "        x_merged = torch.cat([x_gen_exp, x_mirna, x_clinical], -1)\n",
    "        \n",
    "        prognosis = self.fc(x_merged)\n",
    "        \n",
    "        # Output is prognosis (COX LOSS)\n",
    "        # As well as the outputs of the sub-networks (SIMILARITY LOSS)\n",
    "        return {\"prognosis\": prognosis, \"gen_exp\": x_gen_exp, \"mirna\": x_mirna, \"clinical\": x_clinical}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qTfXZDnlioNz"
   },
   "source": [
    "# Data Manipulation Helpers\n",
    "Helper functions for data manipulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "oQh3Tqm5ioN0"
   },
   "outputs": [],
   "source": [
    "# Given a list of datapoints, create a batch\n",
    "def datapoints_to_batch(xs):\n",
    "    sample = xs[0]\n",
    "    out = {}\n",
    "    for key in sample.keys():\n",
    "        if key == \"type\":\n",
    "            continue\n",
    "        lst = []\n",
    "        for x in xs:\n",
    "            lst.append(x[key])\n",
    "        out[key] = torch.stack(lst)\n",
    "    return out\n",
    "\n",
    "# Send datapoint(s) to cuda, if available\n",
    "def datapoints_to_device(x):\n",
    "    y = {}\n",
    "    for key in x.keys():\n",
    "        y[key] = x[key].to(device)\n",
    "    return y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "NS3FmgfLioN5",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "r5RV2lGpeWhI",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from numpy import errstate, isneginf\n",
    "\n",
    "mirna_path = \"./data/miRNA/\"\n",
    "rnaseq_path = \"./data/rnaseq/\"\n",
    "clinical_data_path = \"./data/original_clinical_dataset.json\"\n",
    "# If you want to load a specific dataset only, specify below\n",
    "# For example: 'READ' or 'LUSC'\n",
    "# Empty string: '' for the whole dataset\n",
    "dataset_to_load = ''\n",
    "\n",
    "# Use for miRNA and gene expression only\n",
    "def load_data(path, data_type, data):\n",
    "    for x in os.listdir(path):\n",
    "        if \"data\" not in x:\n",
    "            continue\n",
    "        if dataset_to_load not in x:\n",
    "            continue\n",
    "        data_path = os.path.join(path, x)\n",
    "        cancer_type = x.split(\"_\")[1]\n",
    "        df = pd.read_pickle(data_path, compression=\"gzip\")\n",
    "        # normalize and mean center the data\n",
    "#         with errstate(divide = 'ignore'):\n",
    "#             df = df.apply(lambda x :np.log(x+1))\n",
    "#             df[isneginf(df)] = 0  # solve the negative infinity problem\n",
    "#         # mean center it\n",
    "#         df = df.apply(lambda x: x-x.mean())\n",
    "        for _, row in df.iterrows():\n",
    "            \n",
    "            splitname = row.name.split(\"-\")\n",
    "            patient = \"-\".join([splitname[0], splitname[1], splitname[2]])\n",
    "            if patient not in data:\n",
    "                data[patient] = {}\n",
    "                data[patient]['type'] = None\n",
    "            # WARNING!! TODO\n",
    "            # What to do if a patient has two mirna data?\n",
    "            if data_type in data[patient]:\n",
    "                continue\n",
    "#                 raise Exception(f\"The patient {patient} has two {data_type} specified\")\n",
    "\n",
    "            data[patient][data_type] = torch.tensor(row.values.astype(np_type))\n",
    "            data[patient]['type'] = cancer_type\n",
    "            if data[patient]['type'] == 'and':\n",
    "                data[patient]['type'] = 'HNSC'\n",
    "\n",
    "def load_clinical_data(path, data_type, data):\n",
    "    import json\n",
    "    with open(path, \"r\") as file:\n",
    "        clinical_data = json.load(file)\n",
    "    \n",
    "    races = []\n",
    "    genders = []\n",
    "    for case in clinical_data:\n",
    "        patient = case['demographic']['submitter_id'].split(\"_\")[0]\n",
    "        \n",
    "        # Assumption: clinical data is loaded LAST\n",
    "        # If we only have clinical data, skip the patient\n",
    "        if patient not in data:\n",
    "            continue\n",
    "        \n",
    "        race = case['demographic']['race']\n",
    "        if race not in races:\n",
    "            races.append(race)\n",
    "        race = races.index(race)\n",
    "        \n",
    "        if 'days_to_birth' in case['demographic']:\n",
    "            age = case['demographic']['days_to_birth']\n",
    "        else:\n",
    "            age = -1\n",
    "        \n",
    "        gender = case['demographic']['gender']\n",
    "        if gender not in genders:\n",
    "            genders.append(gender)\n",
    "        gender = genders.index(gender)\n",
    "        \n",
    "        # TODO: HISTOLOGICAL GRADE\n",
    "        \n",
    "        vital_status = case['demographic']['vital_status']\n",
    "        # TODO: what is time to death when someone is alive?\n",
    "        # in the figure reproduction for those patients I used the days to last follow up value\n",
    "        # time_to_death = case['diagnoses']['days_to_last_follow_up']\n",
    "        # but based on cox loss desc they could be just skipped \"neural network model trained to predict survival times. The loss iscomputed  over  all  patients  whose  lack  of  survival  was  observed.\"\n",
    "        if vital_status.lower() == \"alive\":\n",
    "            time_to_death = 1e5\n",
    "        else:\n",
    "            if 'days_to_death' in case['demographic']:\n",
    "                time_to_death = case['demographic']['days_to_death']\n",
    "            else:\n",
    "                # TODO: what is time of death when not specified?\n",
    "                time_to_death = 1e5\n",
    "        \n",
    "        datapoint = np.array([race, age, gender, 0]).astype(np_type)\n",
    "        data[patient]['clinical'] = torch.Tensor(datapoint)\n",
    "        \n",
    "        time_to_death = np.array([time_to_death]).astype(np_type)\n",
    "        data[patient]['prognosis'] = torch.Tensor(time_to_death)\n",
    "\n",
    "def move_to(obj, device):\n",
    "    if torch.is_tensor(obj):\n",
    "        return obj.to(device)\n",
    "    elif isinstance(obj, dict):\n",
    "        res = {}\n",
    "        for k, v in obj.items():\n",
    "            res[k] = move_to(v, device)\n",
    "            if k == 'type':\n",
    "                res[k] = v\n",
    "        return res\n",
    "\n",
    "dataset = {}\n",
    "load_data(mirna_path, 'mirna', dataset)\n",
    "load_data(rnaseq_path, 'gen_exp', dataset)\n",
    "load_clinical_data(clinical_data_path, 'clinical', dataset)\n",
    "types = []\n",
    "for key in dataset.keys():\n",
    "    typ = dataset[key]['type']\n",
    "    if typ not in types:\n",
    "        types.append(typ)\n",
    "print(f\"Loaded patient data with {len(dataset)} patients\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "oxejDIz5ioN8"
   },
   "source": [
    "# Handle Missing Data\n",
    "\n",
    "To handle the missing data the missing fields are filled up with zero values. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Er8_51VSioN9",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "empty_mirna = np.zeros(1881).astype(np_type)\n",
    "empty_mirna = torch.tensor(empty_mirna)\n",
    "\n",
    "empty_gen_exp = np.zeros(60483).astype(np_type)\n",
    "empty_gen_exp = torch.tensor(empty_gen_exp)\n",
    "\n",
    "empty_clinical = np.zeros(4).astype(np_type)\n",
    "empty_clinical = torch.tensor(empty_clinical)\n",
    "\n",
    "# empty_slides = np.zeros([40,3,224,224]).astype(np_type)\n",
    "# empty_slides = torch.tensor(empty_slides)\n",
    "\n",
    "empty_prognosis = np.array([0]).astype(np_type)\n",
    "empty_prognosis = torch.tensor(empty_prognosis)\n",
    "\n",
    "def fill_empty(data, key, value):\n",
    "    if key not in data:\n",
    "        data[key] = value\n",
    "\n",
    "# Given a datapoint, fill it with 0s if one of the modalities is missing\n",
    "def fill_dataset(dataset):\n",
    "    for datapoint in dataset:\n",
    "        fill_empty(dataset[datapoint], 'mirna', empty_mirna)\n",
    "        fill_empty(dataset[datapoint], 'gen_exp', empty_gen_exp)\n",
    "        fill_empty(dataset[datapoint], 'clinical', empty_clinical)\n",
    "#         fill_empty(dataset[datapoint], 'slides', empty_slides)\n",
    "        fill_empty(dataset[datapoint], 'prognosis', empty_prognosis)\n",
    "\n",
    "fill_dataset(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "OcVMglq0JfvV"
   },
   "source": [
    "## Separation of the data to training set and test set\n",
    "\n",
    "To prevent overfitting the dataset is separated to training and test sets it sizes can be adjusted by parameter test_size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 286
    },
    "colab_type": "code",
    "id": "zptQ6NjkJfvV",
    "outputId": "8c9d62a4-9f67-48e6-8784-44fdbbbc33cf",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def create_train_test(dataset,test_size):\n",
    "    data = pd.DataFrame(dataset).T\n",
    "    y = data.pop('prognosis').to_frame()\n",
    "    X = data\n",
    "    traindf, testdf, ytrain, ytest = train_test_split(X,y,test_size=test_size,shuffle=True,stratify=X.loc[:,'type'])\n",
    "    traindf['prognosis'] = ytrain\n",
    "    testdf['prognosis'] = ytest\n",
    "    trainset = traindf.T.to_dict()\n",
    "    testset = testdf.T.to_dict()\n",
    "    return move_to(trainset,device), move_to(testset,device)\n",
    "\n",
    "import time\n",
    "\n",
    "t1 = time.time()\n",
    "\n",
    "test_size = 0.25\n",
    "trainset, testset = create_train_test(dataset,test_size)\n",
    "\n",
    "t2 = time.time()\n",
    "\n",
    "del dataset\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "print(f\"Trainset size: {len(trainset)}\")\n",
    "print(f\"Testset size: {len(testset)}\")\n",
    "print(f\"Data loaded in {t2-t1} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data normalization\n",
    "Two different data manipulation techniques are used for the miRNA and gene expression data.\n",
    "\n",
    "1. The data is normalized using the logarithm function: $f(x) = log(x+1)$\n",
    "2. The data is mean-shifted\n",
    "\n",
    "All the manipulations occur on feature level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(dataset, train_means=None):\n",
    "    yy = ['mirna', 'gen_exp']\n",
    "    means = {}\n",
    "    # First, log(x + 1)\n",
    "    for x in dataset:\n",
    "        for y in yy:\n",
    "            if y not in means:\n",
    "                means[y] = []\n",
    "            dataset[x][y] = torch.log(dataset[x][y] + 1)\n",
    "            means[y].append(dataset[x][y])\n",
    "            \n",
    "    # Now, Calculate means\n",
    "    for y in means:\n",
    "        means[y] = torch.stack(means[y])\n",
    "        means[y] = means[y].mean(dim=0)\n",
    "\n",
    "    # Finally, mean shift\n",
    "    for x in dataset:\n",
    "        for y in yy:\n",
    "            if train_means is None:\n",
    "                dataset[x][y] = dataset[x][y] - means[y]\n",
    "            else:\n",
    "                dataset[x][y] = dataset[x][y] - train_means[y]\n",
    "    \n",
    "    return means\n",
    "\n",
    "means = normalize(trainset)\n",
    "normalize(testset, means)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_Mm6bdTfioN_"
   },
   "source": [
    "# Similarity Loss\n",
    "\n",
    "The purpose of the similarity loss is to create meaningful feature vectors from the different data modalities, by forcing the networks to create similar feature vectors from different data modalities of the same data. The similarity between two feature vectors x and y is defined as follows:\n",
    "\n",
    "\\begin{equation}\n",
    "si m_{\\theta} (x,y) = \\sum _{i,\\in modalities} \\frac{\\hat{h}_{\\theta , i }(x_i)\\cdot \\hat{h}_{\\theta , j} (y_j)}{\\|\\hat{h}_{\\theta , i }(x_i) \\| \\| \\hat{h}_{\\theta , j} (y_j) \\| }\n",
    "\\end{equation}\n",
    "\n",
    "Here $ \\hat{h}_{\\theta , i}(x_j)$ signifies the feature vector produced by network $h$ with parameters $\\theta$ from input data from modility $i$. The formula can be interpreted as the \"angle\" between the two feature vectors which is small when they are very similar and large when they are dissimilar. Moreover, this \"angle\" is summed for all pairs of modalities.\n",
    "The loss for two data points is then: \n",
    "\n",
    "\\begin{equation}\n",
    "L_{\\theta} (x,y) = max(0, M- sim_{\\theta} (x,y)+ sim_{\\theta} (x,x))\n",
    "\\end{equation}\n",
    "\n",
    "This formulation makes sure the loss becomes larger when the vectors are similar between the same patient and dissimilar for different patients. The variable $M$ tunes the realtive importance of the vectors being similar. When $M$ is high, the similarity measure is relatively small so the vectors are allowed to be more dissimilar, and vice versa. The may makes sure the loss cannot become negative.\n",
    "Finally the loss is summed over all patient data:\n",
    "\n",
    "\\begin{equation}\n",
    "l_{sim} (\\theta ) = \\sum _{x,y} L_{\\theta} (x,y)\n",
    "\\end{equation}\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3tVb1CjGkc4w",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Calculate all cosine similarities \n",
    "# Fast because vectorized\n",
    "def sim_matrix(data):\n",
    "    # sim[k][l] is the cosine similarity between patients k and l\n",
    "    sim = None\n",
    "    \n",
    "    # Based on: https://stackoverflow.com/a/50426321\n",
    "    for i in range(len(data)):\n",
    "        for j in range(len(data)):\n",
    "            # Cos similarity\n",
    "            # = U * V / |U||V|\n",
    "            # = (U / |V|) * (V / |V|)\n",
    "\n",
    "            # Add small epsilon for missing modalities\n",
    "            eps = 1e-5\n",
    "\n",
    "            # First, calculate the normalized values (U / |V|) and (V / |V|)\n",
    "            i_norm = data[i] / (data[i].norm(dim=1)[:, None] + eps)\n",
    "            j_norm = data[j] / (data[j].norm(dim=1)[:, None] + eps)\n",
    "\n",
    "            # Now calculate for each patient, the dot product using matrix multiplication\n",
    "            # res[k][l] is cosine similarity of modalities [i][j] for patients [k][l]\n",
    "            res = torch.mm(i_norm, j_norm.transpose(0, 1))\n",
    "\n",
    "            # Initialize the tensor\n",
    "            if (sim is None):\n",
    "                sim = res\n",
    "            # Sum up similarities over different modalities\n",
    "            else:\n",
    "                sim = sim.add(res)\n",
    "    return sim\n",
    "\n",
    "\n",
    "def similarity(data):\n",
    "    M = 0.1\n",
    "\n",
    "    # This is sim(x, y)\n",
    "    sim = sim_matrix(data)\n",
    "    \n",
    "    # The diagonal of the matrix is the self-similarity\n",
    "    # The following holds: simxx(x) = sim(x, x)\n",
    "    simxx = torch.diag(sim)\n",
    "\n",
    "    sim = M - sim + simxx\n",
    "    \n",
    "    sim.clamp(min=0)\n",
    "\n",
    "    x = sim.sum()\n",
    "        \n",
    "    return x\n",
    "\n",
    "\n",
    "def similarity_loss(outputs):\n",
    "    mirna = outputs['mirna']\n",
    "    genexp = outputs['gen_exp']\n",
    "    clinical = outputs['clinical']\n",
    "    \n",
    "    data = [mirna, genexp, clinical]\n",
    "    \n",
    "    return similarity(data) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "oBnpmg87ioOD",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Cox loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wPISyRsZioOD",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def cox_loss(data, labels):\n",
    "    '''\n",
    "    Computes the Cox loss for a batch of patient data for which survival time is available.\n",
    "    pred: survival predictions from the model (torch.tensor.shape = [1,batchsize])\n",
    "    labels: actual survival times (labels) of the data (torch.tensor.shape = [1,batchsize] or [batchsize, 1])\n",
    "    '''\n",
    "    pred = data['prognosis']\n",
    "    dead = labels < (1e5 - 1) \n",
    "    labels = labels[dead]\n",
    "    pred = pred[dead]\n",
    "    labels.squeeze_()\n",
    "    pred.squeeze_()\n",
    "    _, ix = labels.sort()\n",
    "    pred = pred[ix]\n",
    "    pred = pred.float()\n",
    "    loss = torch.tensor([0.]).to(device)\n",
    "    for i in range(len(pred)):\n",
    "        if i < len(pred)-1:\n",
    "            loss -= pred[i] - torch.log( torch.exp(pred[i+1:]).sum())\n",
    "        else:\n",
    "            loss -= pred[i]\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Validation Loop\n",
    "C-index is calculated for the a given dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate(network, loader, dataset):\n",
    "    # Validation\n",
    "    actual = None\n",
    "    predicted = None\n",
    "    for data_keys in loader:\n",
    "        data = list(map(lambda x: dataset[x], data_keys))\n",
    "        data = datapoints_to_batch(data)\n",
    "        with torch.no_grad():\n",
    "            output = network(data)\n",
    "            if actual is None:\n",
    "                actual = data['prognosis']\n",
    "                predicted = output['prognosis']\n",
    "            else:\n",
    "                actual = torch.cat((actual, data['prognosis']))\n",
    "                predicted = torch.cat((predicted, output['prognosis']))\n",
    "\n",
    "\n",
    "    actual = np.squeeze(np.array(actual.cpu()))\n",
    "    predicted = np.squeeze(np.array(predicted.cpu()))\n",
    "    \n",
    "    # Mask is to exclude patients that did not die yet\n",
    "    mask = actual < (1e5 - 1)\n",
    "    c = lifelines.utils.concordance_index(actual, -predicted, mask)\n",
    "    return c"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "l8ZrGaraioOG",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Training Loop\n",
    "Completely different than the paper right now, but shows that the network defined above can be trained. Note that the network wont learn anything since training is on random data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "ftvoSYMEJfvo",
    "outputId": "d8d9f14c-bb1d-4646-d536-5267157dcd27",
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import lifelines\n",
    "import torch.optim as optim\n",
    "import copy\n",
    "\n",
    "batch_size = 512\n",
    "epochs = 25\n",
    "multimodal_dropout = False\n",
    "\n",
    "if (multimodal_dropout):\n",
    "    p = 0.25\n",
    "else:\n",
    "    p = 0\n",
    "\n",
    "network = Network(p=p).to(device)\n",
    "optimizer = optim.AdamW(network.parameters(), lr=0.00001, amsgrad=True)\n",
    "\n",
    "data_loader = torch.utils.data.DataLoader(list(trainset.keys()), batch_size=batch_size, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(list(testset.keys()), batch_size=batch_size, shuffle=False)\n",
    "\n",
    "losses = [[] for epoch in range(epochs)]\n",
    "losses_sim = [[] for epoch in range(epochs)]\n",
    "losses_cox = [[] for epoch in range(epochs)]\n",
    "\n",
    "C_test = []\n",
    "C_data = []\n",
    "\n",
    "counter = 0\n",
    "\n",
    "best_network_state_dict = None\n",
    "best_network_c_test = 0.0\n",
    "\n",
    "for epoch in range(epochs): # Epochs\n",
    "    for data_keys in data_loader: # Patients\n",
    "        optimizer.zero_grad()\n",
    "        counter += 1\n",
    "\n",
    "        data = list(map(lambda x: trainset[x], data_keys))\n",
    "        data = datapoints_to_batch(data)\n",
    "        \n",
    "        labels = data['prognosis']\n",
    "\n",
    "        outputs = network(data)\n",
    "\n",
    "        loss_sim = similarity_loss(outputs)\n",
    "        loss_cox = cox_loss(outputs, labels)\n",
    "        \n",
    "        ratio = loss_cox / loss_sim\n",
    "        ratio = ratio.detach()\n",
    "        \n",
    "        loss_sim = loss_sim * ratio * 0.1\n",
    "        \n",
    "        loss_total = loss_sim + loss_cox\n",
    "        loss_total.backward()\n",
    "        \n",
    "        optimizer.step()\n",
    "        \n",
    "        losses_sim[epoch].append(loss_sim.item())\n",
    "        losses_cox[epoch].append(loss_cox.item())\n",
    "        losses[epoch].append(loss_total.item())\n",
    "\n",
    "    c_test = validate(network, test_loader, testset)\n",
    "    c_data = validate(network, data_loader, trainset)\n",
    "    C_test.append(c_test)\n",
    "    C_data.append(c_data)\n",
    "    counter = 0\n",
    "    print(f\"Epoch: {epoch} C-score test: {c_test}, C-score data: {c_data}\")\n",
    "    \n",
    "    if c_test > best_network_c_test:\n",
    "        best_network_state_dict = copy.deepcopy(network.state_dict())\n",
    "        best_network_c_test = c_test\n",
    "        print(f\"Found new best C-score of: {c_test}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Various plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "output_dir = \"./output\"\n",
    "\n",
    "try:\n",
    "    os.mkdir(output_dir)\n",
    "except:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 67
    },
    "colab_type": "code",
    "id": "OVSNPMbCitGc",
    "outputId": "53cf525a-8d3e-4b64-9b07-7a093eb09667"
   },
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure()\n",
    "total = np.mean(np.array(losses), axis=1)\n",
    "plt.plot(total)\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Total loss\")\n",
    "plt.savefig(f\"{output_dir}/loss.svg\", dpi=1000)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### C-index curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "p1SpbhIEJfvq",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "\n",
    "plt.plot(C_test)\n",
    "plt.plot(C_data)\n",
    "plt.ylim(0.65, 0.85)\n",
    "plt.legend([\"C-index test\", \"C-index train\"])\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"C-index\")\n",
    "plt.savefig(f\"{output_dir}/default.svg\", dpi=1000)\n",
    "\n",
    "print(max(C_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### C-index per cancer site"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sort_per_type(dataset):\n",
    "    types = []\n",
    "    for key in dataset.keys():\n",
    "        typ = dataset[key]['type']\n",
    "        if typ not in types:\n",
    "            types.append(typ)\n",
    "    data_per_type = {typ:{} for typ in types}\n",
    "    for key in dataset.keys():\n",
    "        data_per_type[dataset[key]['type']][key] = dataset[key]\n",
    "    return data_per_type\n",
    "\n",
    "def validate_per_type(network, dataset):\n",
    "    sorted_data = sort_per_type(dataset)\n",
    "    loaders = {}\n",
    "    for typ, data in sorted_data.items():\n",
    "        loaders[typ] = torch.utils.data.DataLoader(list(data.keys()), batch_size=batch_size, shuffle=True)\n",
    "    # Validation\n",
    "    C = {}\n",
    "    for typ, loader in loaders.items():\n",
    "        actual = None\n",
    "        predicted = None\n",
    "        for data_keys in loader:\n",
    "            data = list(map(lambda x: dataset[x], data_keys))\n",
    "            data = datapoints_to_batch(data)\n",
    "            with torch.no_grad():\n",
    "                output = network(data)\n",
    "                if actual is None:\n",
    "                    actual = data['prognosis']\n",
    "                    predicted = output['prognosis']\n",
    "                else:\n",
    "                    actual = torch.cat((actual, data['prognosis']))\n",
    "                    predicted = torch.cat((predicted, output['prognosis']))\n",
    "\n",
    "        actual = np.squeeze(np.array(actual.cpu()))\n",
    "        predicted = np.squeeze(np.array(predicted.cpu()))\n",
    "        \n",
    "        # Mask is to exclude patients that did not die yet\n",
    "        mask = actual < (1e5 - 1)\n",
    "        try:\n",
    "            c = lifelines.utils.concordance_index(actual, -predicted, mask)\n",
    "            C[typ] = c\n",
    "        except:\n",
    "            print(typ)\n",
    "    return C\n",
    "\n",
    "network.load_state_dict(best_network_state_dict)\n",
    "validate_per_type(network, testset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def c_per_type(network, dataset):\n",
    "    allowed_types = [\"BLCA\", \"BRCA\", \"CESC\", \"COAD\", \"READ\", \"HNSC\", \"KICH\", \"KIRC\", \"KIRP\", \"LAML\", \"LGG\", \"LIHC\", \"LUAD\", \"LUSC\", \"OV\", \"PAAD\", \"PRAD\", \"SKCM\", \"STAD\", \"THCA\", \"UCEC\"]\n",
    "    types = []\n",
    "    C = []\n",
    "    for key in dataset.keys():\n",
    "        typ = dataset[key]['type']\n",
    "        if typ not in types:\n",
    "            types.append(typ)\n",
    "    C = {}\n",
    "    with torch.no_grad():\n",
    "        data = datapoints_to_batch(list(dataset.values()))\n",
    "        output = network(data)\n",
    "\n",
    "        actual = data['prognosis']\n",
    "        predicted = output['prognosis']\n",
    "\n",
    "        actual = np.squeeze(np.array(actual.cpu()))\n",
    "        predicted = np.squeeze(np.array(predicted.cpu()))\n",
    "        \n",
    "        for typ in types:\n",
    "#             if typ not in allowed_types:\n",
    "#                 continue\n",
    "\n",
    "            mask_type = list(map(lambda x: x['type'] == typ, dataset.values()))\n",
    "            mask = actual < (1e5 - 1)\n",
    "            try:\n",
    "                c = lifelines.utils.concordance_index(actual[mask_type], -predicted[mask_type], mask[mask_type])\n",
    "                C[typ] = c\n",
    "            except:\n",
    "                pass\n",
    "    return C\n",
    "\n",
    "network.load_state_dict(best_network_state_dict)\n",
    "\n",
    "Cs = c_per_type(network, testset)\n",
    "for x in Cs:\n",
    "    print(f\"{x} {round(Cs[x], 3)}\")\n",
    "mn = np.mean(np.array(list(Cs.values())))\n",
    "print(mn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "machine_shape": "hm",
   "name": "multimodal-prognosis.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
